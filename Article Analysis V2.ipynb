{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from statsmodels.tsa import stattools\n",
    "import pickle\n",
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.externals.joblib import dump, load\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded=files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove(\"stock_price.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_obj=zipfile.ZipFile(\"stock_price.zip\",\"r\",zipfile.ZIP_DEFLATED)\n",
    "zip_obj.extractall()\n",
    "zip_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_adj_close_price_data(directory=\"./historical_data/\"):\n",
    "    \"\"\"\n",
    "    Retrieve daily adj close prices of all SP500 index constituent stocks and concatenate them into a DataFrame object\n",
    "    \"\"\"\n",
    "    adj_price_list=[]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename in [\".DS_Store\",\"stock_price.zip\"]:\n",
    "            continue\n",
    "        adj_close=pd.read_csv(directory+filename,index_col=\"Date\",parse_dates=True)[[\"Adj Close\"]]\n",
    "        adj_close.columns=pd.Index([filename.split(\"+\")[0]])\n",
    "        adj_price_list.append(adj_close)\n",
    "    return pd.concat(adj_price_list,axis=1,join='outer')\n",
    "\n",
    "adj_close_price=retrieve_adj_close_price_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_close_price.diff(1)/adj_close_price.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net_return_in_n_day_period(close_price,n_periods=1):\n",
    "    \"\"\"\n",
    "    Calculate simple net return in a n-day period with formula R(t,n)=(P(t)-P(t-n))/P(t-n)\n",
    "    \"\"\"\n",
    "    simple_return=close_price.diff(n_periods)/close_price.shift(n_periods)\n",
    "    return simple_return[n_periods:]\n",
    "\n",
    "daily_return=simple_net_return_in_n_day_period(adj_close_price,1)\n",
    "\n",
    "# the number of non-missing value of  every stock\n",
    "daily_return.describe().loc[\"count\"].hist(bins=20)\n",
    "\n",
    "# the ticker of stock with smallest non-missing value\n",
    "ticker=np.argmin(daily_return.describe().loc[\"count\"])\n",
    "count=int(np.min(daily_return.describe().loc[\"count\"]))\n",
    "print(\"Stock {} with the smallest non-missing value, count = {}.\".format(ticker,count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abnormal_series_detection(daily_return):\n",
    "    \"\"\"\n",
    "    Detect abnormal stock daily return time series, using the number of transitions from missing value (i.e. np.nan)\n",
    "    to non-missing value or the reverse, since the normal transition number should be 0 (for stock listed since the \n",
    "    start of study period), 1 (for stock listed at start but then delisted OR stock not listed at start but then\n",
    "    listed) or 2 (for stock not listed at start but then listed and finally delisted; another possibility belongs to\n",
    "    abnormal, i.e. for stock listed at start but then delisted and finally listed again). And abnormal time series\n",
    "    with more than 2 transitions may be related with suspending trading or other situations.\n",
    "    \"\"\"\n",
    "    abnormal=daily_return.isna().astype(np.int32).diff(1)[1:].abs().sum(axis=0)  # a Series with length = stock_num\n",
    "    abnormal_stocks=list(abnormal[abnormal>=2].index)\n",
    "    num_rows=len(abnormal_stocks)//3+1\n",
    "    plt.figure(figsize=(20,num_rows*4))\n",
    "    for i,stock in enumerate(abnormal_stocks):\n",
    "        plt.subplot(num_rows,3,i+1)\n",
    "        plt.plot(daily_return[stock])\n",
    "        plt.title(\"{}'s daily simple net return with transitions = {}\".format(stock,int(abnormal[stock])))\n",
    "\n",
    "abnormal_series_detection(daily_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_constituent_stock_matrix(daily_return_m):\n",
    "    \"\"\"\n",
    "    Generate sample SP500 constituent stock binary matrix with element (i,j) denoting whether the j-th stock belongs\n",
    "    to SP500 index at i-th month end (0=False, 1=True) before obtaining the actual constituent stock matrix\n",
    "    \"\"\"\n",
    "    start=str(daily_return_m.index[0].year)+\"-\"+str(daily_return_m.index[0].month)\n",
    "    end=str(daily_return_m.index[-1].year)+\"-\"+str(daily_return_m.index[-1].month)\n",
    "    date_idx=pd.date_range(start,end,freq=\"M\")\n",
    "    stock_ticker=daily_return_m.columns\n",
    "    return pd.DataFrame(np.ones((len(date_idx),len(stock_ticker)),np.int16),index=date_idx,columns=stock_ticker)\n",
    "\n",
    "stock_consti=generate_sample_constituent_stock_matrix(daily_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_study_periods(total_days,train_period,trade_period):\n",
    "    \"\"\"\n",
    "    Calculate the number of study periods given total available days, train period and trade period length\n",
    "    \"\"\"\n",
    "    return (total_days-train_period)//trade_period\n",
    "\n",
    "train_period=750\n",
    "trade_period=250\n",
    "sequence_length=240\n",
    "num_study_period=calculate_num_study_periods(len(daily_return.index),train_period,trade_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_one_month_ahead(time_point):\n",
    "    \"\"\"\n",
    "    Shift the timestamp to the previous month's ending\n",
    "    \"\"\"\n",
    "    return pd.Timestamp(str(time_point.year)+\"-\"+str(time_point.month)+\"-\"+\"01\",freq=\"M\")-pd.Timedelta(1,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_value(input_array):\n",
    "    \"\"\"\n",
    "    Generate target value for input array with those stocks the return of which is larger than the median level set \n",
    "    to positive value and the other stocks the return of which is smaller than the median level set to negative value\n",
    "    at each current timestep\n",
    "    \"\"\"\n",
    "    input_array[:,:,-1]=input_array[:,:,-1]-np.nanmedian(input_array[:,:,-1],axis=1,keepdims=True)\n",
    "    return input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_missing_value(input_array):\n",
    "    \"\"\"\n",
    "    Exclude missing value (including missing return information at pervious timesteps or missing target value) and \n",
    "    flatten 3-D array (num_timesteps, num_stocks, sequence_len) into 2-D array (num_instances, sequence_len)\n",
    "    \"\"\"\n",
    "    nan_matrix=(np.sum(np.isnan(input_array),axis=2)==0)\n",
    "    nan_ratio=100-np.sum(nan_matrix)/(nan_matrix.shape[0]*nan_matrix.shape[1])*100\n",
    "    print(\"There are {:.2f}% missing values in constructed sequences!\".format(nan_ratio))\n",
    "    exclude_nan=input_array[nan_matrix]\n",
    "    return exclude_nan,nan_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_trade_set_each_study_period(return_m,stock_m,train_period,trade_period,sequence_len,\n",
    "                                               study_period_idx,save_directory=\"./train_trade_set\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # obtain the up-to-date (previous month end) list of SP500 constituent stocks at the last day of train period\n",
    "    train_period_end_idx=study_period_idx*trade_period+train_period-1\n",
    "    train_period_end_date=return_m.index[train_period_end_idx]\n",
    "    train_period_end_prev_month=shift_one_month_ahead(train_period_end_date)\n",
    "    stock_list=stock_m.iloc[stock_m.index.get_loc(train_period_end_prev_month)] # a Series with length = stock_num\n",
    "    \n",
    "    # obtain the return series of SP500 constituent stocks (may have slight diff to reality) in this study period\n",
    "    start_idx=study_period_idx*trade_period\n",
    "    end_idx=(study_period_idx+1)*trade_period+train_period\n",
    "    return_sp500=return_m[stock_list[stock_list==1].index].iloc[start_idx:end_idx]\n",
    "        \n",
    "    # standardize return series using train set's mean return and stdev\n",
    "    return_train_array=return_sp500.iloc[:train_period].values\n",
    "    miu=np.nanmean(return_train_array)\n",
    "    sigma=np.nanstd(return_train_array)\n",
    "    return_std=(return_sp500-miu)/sigma\n",
    "    \n",
    "    # transform return series from DataFrame to Ndarray and add the timesteps dimension (axis=2)\n",
    "    return_array=return_std.values\n",
    "    return_column=return_std.columns\n",
    "    return_add_dims=np.expand_dims(return_array,axis=2)\n",
    "    \n",
    "    # construct train and trade set with return information at pervious \"sequence_len\" timesteps\n",
    "    shifted_list=[return_add_dims]\n",
    "    for t in range(sequence_len):\n",
    "        shifted_list.append(np.roll(return_add_dims,shift=t+1,axis=0))\n",
    "    shifted_list_r=list(reversed(shifted_list))\n",
    "    return_concat=np.concatenate(shifted_list_r,axis=2)\n",
    "    return_train_set=return_concat[sequence_len:train_period,:,:]\n",
    "    return_trade_set=return_concat[train_period:,:,:]\n",
    "        \n",
    "    # generate target value (positive when outperform median level and negative when underperform median level)\n",
    "    return_train_set_label=generate_target_value(return_train_set)\n",
    "    return_trade_set_label=generate_target_value(return_trade_set)\n",
    "    \n",
    "    # print(return_train_set_label[:,:,-1])\n",
    "    # print(return_trade_set_label[:,:,-1])\n",
    "    \n",
    "    # exclude missing value and flatten 3-D array into 2-D array\n",
    "    return_train_set_nonan,_=exclude_missing_value(return_train_set_label)\n",
    "    return_trade_set_nonan,nonan_identifier=exclude_missing_value(return_trade_set_label)  \n",
    "    \n",
    "    # separate features (X) and target (Y) and transform target in binary value (1 = positive, 0 = negative)\n",
    "    return_train_set_X=return_train_set_nonan[:,:-1]\n",
    "    return_train_set_Y=(return_train_set_nonan[:,-1]>=0).astype(np.int32)\n",
    "    return_trade_set_X=return_trade_set_nonan[:,:-1]\n",
    "    return_trade_set_Y=(return_trade_set_nonan[:,-1]>=0).astype(np.int32)\n",
    "     \n",
    "    # save train, trade set and non-nan identifier of trade set using pickle binary format \n",
    "    pickle.dump(return_train_set_X,open(save_directory+\"/return_train_X_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(return_train_set_Y,open(save_directory+\"/return_train_Y_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(return_trade_set_X,open(save_directory+\"/return_trade_X_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(return_trade_set_Y,open(save_directory+\"/return_trade_Y_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(nonan_identifier,open(save_directory+\"/nonan-identifier_for_trade_\"+str(study_period_idx),\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_trade_set_each_study_period_benchmark(adj_close_price,stock_m,train_period,trade_period,\n",
    "                                                         study_period_idx,save_directory=\"./train_trade_set\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # obtain the up-to-date (previous month end) list of SP500 constituent stocks at the last day of train period\n",
    "    train_period_end_idx=study_period_idx*trade_period+train_period-1\n",
    "    train_period_end_date=adj_close_price[1:].index[train_period_end_idx]\n",
    "    train_period_end_prev_month=shift_one_month_ahead(train_period_end_date)\n",
    "    stock_list=stock_m.iloc[stock_m.index.get_loc(train_period_end_prev_month)] # a Series with length = stock_num\n",
    "    \n",
    "    # obtain the adj close price series of SP500 constituent stocks (may have slight diff to reality) in this study period\n",
    "    start_idx=study_period_idx*trade_period\n",
    "    end_idx=(study_period_idx+1)*trade_period+train_period+1\n",
    "    price_sp500=adj_close_price[stock_list[stock_list==1].index].iloc[start_idx:end_idx]\n",
    "    \n",
    "    # calculate n_period return to construct input features\n",
    "    return_list=[simple_net_return_in_n_day_period(price_sp500,1)]\n",
    "    for n_periods in list(np.linspace(1,20,20))+list(np.linspace(40,240,11)):\n",
    "        return_n_period=simple_net_return_in_n_day_period(price_sp500,int(n_periods))\n",
    "        return_list.append(return_n_period.shift(1,axis=0))\n",
    "    return_frame=pd.concat(list(reversed(return_list)),axis=1,join=\"outer\")\n",
    "    \n",
    "    # construct train and trade set with n_period return information at pervious timesteps    \n",
    "    num_stock=np.sum(stock_list==1)\n",
    "    shifted_list=[]\n",
    "    for idx in range(len(return_frame.columns)//num_stock):\n",
    "        return_array=return_frame.iloc[:,idx*num_stock:(idx+1)*num_stock].values\n",
    "        # standardize return series using train set's mean return and stdev\n",
    "        miu=np.nanmean(return_array[:train_period])\n",
    "        sigma=np.nanstd(return_array[:train_period])\n",
    "        return_std=np.expand_dims((return_array-miu)/sigma,axis=2)\n",
    "        shifted_list.append(return_std)\n",
    "    return_concat=np.concatenate(shifted_list,axis=2)\n",
    "    return_train_set=return_concat[240:train_period,:,:]\n",
    "    return_trade_set=return_concat[train_period:,:,:]\n",
    "        \n",
    "    # generate target value (positive when outperform median level and negative when underperform median level)\n",
    "    return_train_set_label=generate_target_value(return_train_set)\n",
    "    return_trade_set_label=generate_target_value(return_trade_set)\n",
    "    \n",
    "    # print(return_train_set_label[:,:,-1])\n",
    "    # print(return_trade_set_label[:,:,-1])\n",
    "    \n",
    "    # exclude missing value and flatten 3-D array into 2-D array\n",
    "    return_train_set_nonan,_=exclude_missing_value(return_train_set_label)\n",
    "    return_trade_set_nonan,nonan_identifier=exclude_missing_value(return_trade_set_label)  \n",
    "    \n",
    "    # separate features (X) and target (Y) and transform target in binary value (1 = positive, 0 = negative)\n",
    "    return_train_set_X=return_train_set_nonan[:,:-1]\n",
    "    return_train_set_Y=(return_train_set_nonan[:,-1]>=0).astype(np.int32)\n",
    "    return_trade_set_X=return_trade_set_nonan[:,:-1]\n",
    "    return_trade_set_Y=(return_trade_set_nonan[:,-1]>=0).astype(np.int32)\n",
    "     \n",
    "    # save train, trade set and non-nan identifier of trade set using pickle binary format \n",
    "    pickle.dump(return_train_set_X,open(save_directory+\"/return_train_X_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(return_train_set_Y,open(save_directory+\"/return_train_Y_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(return_trade_set_X,open(save_directory+\"/return_trade_X_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(return_trade_set_Y,open(save_directory+\"/return_trade_Y_\"+str(study_period_idx),\"wb\"))\n",
    "    pickle.dump(nonan_identifier,open(save_directory+\"/nonan-identifier_for_trade_\"+str(study_period_idx),\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_rnn=\"./train_trade_set/LSTM_RNN\"\n",
    "os.makedirs(data_directory_rnn,exist_ok=True)\n",
    "\n",
    "for i in range(num_study_period):\n",
    "    print(\"Generating train and trade set of LSTM RNN for {}-th study period...\".format(i))\n",
    "    generate_train_trade_set_each_study_period(daily_return,stock_consti,train_period,trade_period,sequence_length,i,\n",
    "                                               data_directory_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_dnn=\"./train_trade_set/benchmark\"\n",
    "os.makedirs(data_directory_dnn,exist_ok=True)\n",
    "\n",
    "for i in range(num_study_period):\n",
    "    print(\"Generating train and trade set of benchmark models for {}-th study period...\".format(i))\n",
    "    generate_train_trade_set_each_study_period_benchmark(adj_close_price,stock_consti,train_period,trade_period,i,\n",
    "                                                         data_directory_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rnn_model(sequence_length):\n",
    "    X_input=keras.layers.Input(shape=(sequence_length,1))\n",
    "    X=keras.layers.CuDNNLSTM(25)(X_input)\n",
    "    X=keras.layers.Dropout(0.1)(X)\n",
    "    X=keras.layers.Dense(1,activation=\"sigmoid\")(X)\n",
    "    model=keras.models.Model(inputs=X_input,outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(train_X,train_Y,trade_X,trade_Y,sequence_len,study_period_idx,to_directory):\n",
    "    \"\"\"\n",
    "    Construct, compile, train LSTM RNN model and then use it to predict\n",
    "    \"\"\"\n",
    "    # construct LSTM RNN model\n",
    "    keras.backend.clear_session()\n",
    "    model=construct_rnn_model(sequence_len)\n",
    "    \n",
    "    # compile LSTM RNN model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.001),loss=keras.losses.binary_crossentropy,metrics=[\"accuracy\"])\n",
    "    \n",
    "    # fit(train) LSTM RNN model\n",
    "    model.fit(train_X,train_Y,batch_size=1000,epochs=1000,verbose=0,validation_split=0.2,callbacks=\n",
    "              [keras.callbacks.EarlyStopping(\"val_loss\",patience=50,restore_best_weights=True),keras.callbacks.\n",
    "               ModelCheckpoint(to_directory+\"/study-\"+str(study_period_idx)+\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                               monitor=\"val_loss\",save_best_only=True)])\n",
    "    \n",
    "    # evaluate LSTM RNN model\n",
    "    model.evaluate(trade_X,trade_Y)\n",
    "    \n",
    "    # generate probability predictions of trade set\n",
    "    pred_prob=model.predict(trade_X)\n",
    "    \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dnn_model(layer_dims):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_input=keras.layers.Input(shape=(layer_dims[0],))\n",
    "    \n",
    "    # the first hidden layer\n",
    "    X=keras.layers.Dense(layer_dims[1],activation=\"relu\",kernel_regularizer=keras.regularizers.l1(0.00001))(X_input)\n",
    "    X=keras.layers.Dropout(0.5)(X)\n",
    "    \n",
    "    # the second hidden layer\n",
    "    X=keras.layers.Dense(layer_dims[2],activation=\"relu\",kernel_regularizer=keras.regularizers.l1(0.00001))(X)\n",
    "    X=keras.layers.Dropout(0.5)(X)\n",
    "    \n",
    "    # the third hidden layer\n",
    "    X=keras.layers.Dense(layer_dims[3],activation=\"relu\",kernel_regularizer=keras.regularizers.l1(0.00001))(X)\n",
    "    X=keras.layers.Dropout(0.5)(X) \n",
    "    \n",
    "    # the output layer\n",
    "    X=keras.layers.Dense(layer_dims[4],activation=None,kernel_regularizer=keras.regularizers.l1(0.00001))(X)\n",
    "    \n",
    "    model=keras.models.Model(inputs=X_input,outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(train_X,train_Y,trade_X,trade_Y,study_period_idx,to_directory):\n",
    "    \"\"\"\n",
    "    Construct, compile, train DNN model and then use it to predict\n",
    "    \"\"\"\n",
    "    # construct DNN model\n",
    "    keras.backend.clear_session()\n",
    "    model=construct_dnn_model([31,31,10,5,1])\n",
    "    \n",
    "    # compile DNN model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.001),loss=keras.losses.binary_crossentropy,metrics=[\"accuracy\"])\n",
    "    \n",
    "    # fit(train) DNN model\n",
    "    model.fit(train_X,train_Y,batch_size=1000,epochs=1000,verbose=0,validation_split=0.2,callbacks=\n",
    "              [keras.callbacks.EarlyStopping(\"val_loss\",patience=50),keras.callbacks.\n",
    "               ModelCheckpoint(to_directory+\"/study-\"+str(study_period_idx)+\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                               monitor=\"val_loss\",save_best_only=True)]) # restore_best_weights=True\n",
    "    \n",
    "    # evaluate LSTM RNN model\n",
    "    model.evaluate(trade_X,trade_Y)\n",
    "    \n",
    "    # generate probability predictions of trade set\n",
    "    pred_prob=model.predict(trade_X)\n",
    "    \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train_X,train_Y,trade_X,study_period_idx,to_directory):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    rf_clf=sklearn.ensemble.RandomForestClassifier(n_estimators=1000,max_depth=20,bootstrap=True)\n",
    "    rf_clf.fit(train_X,train_Y)\n",
    "    pred_prob=rf_clf.predict_proba(trade_X)\n",
    "    \n",
    "    # save model\n",
    "    dump(rf_clf,to_directory+\"/study-\"+str(study_period_idx))\n",
    "    \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_X,train_Y,trade_X,study_period_idx,to_directory):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    log_reg=LogisticRegression(penalty=\"l2\",solver=\"lbfgs\",max_iter=100)\n",
    "    param_grid={\"C\":list(np.power(10,np.linspace(-4,4,101)))}\n",
    "    grid_search=GridSearchCV(log_reg,param_grid,cv=5,scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(train_X,train_Y)\n",
    "    pred_prob=grid_search.best_estimator_.predict_proba(trade_X)[:,1]\n",
    "    \n",
    "    # save model\n",
    "    dump(grid_search.best_estimator_,to_directory+\"/study-\"+str(study_period_idx))\n",
    "    \n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_the_2k_portfolio(pred_prob_frame,k):\n",
    "    portfolio=[]\n",
    "    for i in range(len(pred_prob_frame.index)):\n",
    "        count_nonan=np.sum(1-pred_prob_frame.iloc[0].isna().astype(np.int16)) # count the number of non-nan\n",
    "        if count_nonan>=2*k:\n",
    "            warning=\"nonan>=2k\"\n",
    "        elif (count_nonan<2*k) & (count_nonan>=k):\n",
    "            warning=\"k<=nonan<2k\"\n",
    "            # print(\"Warning: not enough available stocks at date {}, k<=nonan<2k\".format(pred_prob_frame.index[i]))\n",
    "        else:\n",
    "            warning=\"nonan<k\"\n",
    "            # print(\"Serious Warning: not enough available stocks at date {}, nonan<k\".format(pred_prob_frame.index[i]))\n",
    "        pred_prob_frame.sort_values(by=pred_prob_frame.index[i],axis=1,inplace=True,na_position=\"first\")\n",
    "        top_k=list(pred_prob_frame.columns[-k:])\n",
    "        pred_prob_frame.sort_values(by=pred_prob_frame.index[i],axis=1,inplace=True,na_position=\"last\")\n",
    "        flop_k=list(pred_prob_frame.columns[:k])\n",
    "        portfolio.append({\"top_k\":top_k,\"flop_k\":flop_k,\"warning\":warning})\n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_the_realized_return(portfolio,daily_return,adj_close):\n",
    "    \"\"\"\n",
    "    for i in range(len(portfolio)):\n",
    "        # note that the i-th date in adj_close is one day earlier of the i-th date in daily_return\n",
    "        price_long=adj_close.iloc[i][portfolio[i][\"top_k\"]].sum()\n",
    "        price_short=adj_close.iloc[i][portfolio[i][\"flop_k\"]].sum()\n",
    "        weights_long=adj_close.iloc[i][portfolio[i][\"top_k\"]]/(price_long-price_short)\n",
    "        rtn_from_long=(daily_return.iloc[i][portfolio[i][\"top_k\"]]*weights_long).sum()\n",
    "        weights_short=-adj_close.iloc[i][portfolio[i][\"flop_k\"]]/(price_long-price_short)\n",
    "        rtn_from_short=(daily_return.iloc[i][portfolio[i][\"flop_k\"]]*weights_short).sum()\n",
    "        total_rtn=rtn_from_long+rtn_from_short\n",
    "        if ((price_long-price_short<=0) & (total_rtn>0)) | ((price_long-price_short<0) & (total_rtn>=0)):\n",
    "            portfolio[i][\"arbitrage\"]=\"Yes\"\n",
    "        portfolio[i][\"return\"]=total_rtn\n",
    "    return portfolio\n",
    "    \"\"\"\n",
    "  \n",
    "    for i in range(len(portfolio)):\n",
    "        rtn_from_long=daily_return.iloc[i][portfolio[i][\"top_k\"]].mean()\n",
    "        rtn_from_short=-daily_return.iloc[i][portfolio[i][\"flop_k\"]].mean()\n",
    "        portfolio[i][\"return_long\"]=rtn_from_long\n",
    "        portfolio[i][\"return_short\"]=rtn_from_short\n",
    "    return portfolio\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_portfolio_and_return(daily_return,adj_close,study_period_idx,sequence_len,model_type=\"RNN\",\n",
    "                                  from_directory=\"./train_trade_set\",to_directory=\"./model_weights\"):\n",
    "    \n",
    "    # retrieve train and trade set for current study period\n",
    "    train_X=pickle.load(open(from_directory+\"/return_train_X_\"+str(study_period_idx),\"rb\"))\n",
    "    train_Y=pickle.load(open(from_directory+\"/return_train_Y_\"+str(study_period_idx),\"rb\"))\n",
    "    trade_X=pickle.load(open(from_directory+\"/return_trade_X_\"+str(study_period_idx),\"rb\"))\n",
    "    trade_Y=pickle.load(open(from_directory+\"/return_trade_Y_\"+str(study_period_idx),\"rb\"))\n",
    "    \n",
    "    # retrieve non-nan identifier for trade set\n",
    "    nonan_idf=pickle.load(open(from_directory+\"/nonan-identifier_for_trade_\"+str(study_period_idx),\n",
    "                               \"rb\")).astype(np.float64)\n",
    "    \n",
    "    if model_type == \"RNN\":\n",
    "        train_X=np.expand_dims(train_X,axis=2)\n",
    "        train_Y=np.expand_dims(train_Y,axis=2)\n",
    "        trade_X=np.expand_dims(trade_X,axis=2)\n",
    "        trade_Y=np.expand_dims(trade_Y,axis=2)        \n",
    "        pred_prob=lstm_rnn(train_X,train_Y,trade_X,trade_Y,sequence_len,study_period_idx,to_directory)\n",
    "    \n",
    "    elif model_type == \"DNN\":\n",
    "        pred_prob=dnn(train_X,train_Y,trade_X,trade_Y,study_period_idx,to_directory)\n",
    "    \n",
    "    elif model_type == \"RAF\":\n",
    "        pred_prob=random_forest(train_X,train_Y,trade_X,study_period_idx,to_directory)\n",
    "    \n",
    "    elif model_type == \"LOG\":\n",
    "        pred_prob=logistic_regression(train_X,train_Y,trade_X,study_period_idx,to_directory)\n",
    "    \n",
    "    else:\n",
    "        print(\"Model type must belong to ['RNN','DNN','RF','LOG']!\")\n",
    "        return None\n",
    "    \n",
    "    # using non-nan identifier to transform predictions from ndarray with shape (num_instances,1) into DataFrame with\n",
    "    # shape (trade_period,num_stocks) and nan value denoting unavailable stocks or unavailable predictions \n",
    "    nonan_idf[nonan_idf==0]=np.nan\n",
    "    nonan_idf[nonan_idf==1]=pred_prob.reshape(-1)\n",
    "    pred_prob_with_nan=nonan_idf\n",
    "    pred_prob_frame=pd.DataFrame(pred_prob_with_nan,index=daily_return.index,columns=daily_return.columns)\n",
    "    \n",
    "    portfolio_dict={}\n",
    "    for k in [10,50,100,150,200]:\n",
    "        portfolio=construct_the_2k_portfolio(pred_prob_frame,k)\n",
    "        portfolio=obtain_the_realized_return(portfolio,daily_return,adj_close)\n",
    "        portfolio_dict[k]=portfolio\n",
    "    \n",
    "    return portfolio_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory_rnn=\"./model_weights/recurrent_neural_network\"\n",
    "os.makedirs(model_directory_rnn,exist_ok=True)\n",
    "\n",
    "portfolio_list_rnn=[]\n",
    "for i in range(num_study_period):\n",
    "    print(\"Generating the daily 2k portfolio and corresponding returns of LSTM RNN for {}-th study period...\".format(i))\n",
    "    trade_start=i*trade_period+train_period\n",
    "    trade_end=(i+1)*trade_period+train_period\n",
    "    portfolio_dict=generate_portfolio_and_return(daily_return.iloc[trade_start:trade_end],\n",
    "                                                 adj_close_price[trade_start:trade_end],i,sequence_length,\n",
    "                                                 \"RNN\",data_directory_rnn,model_directory_rnn)\n",
    "    portfolio_list_rnn.append(portfolio_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory=\"./analysis_results\"\n",
    "os.makedirs(results_directory,exist_ok=True)\n",
    "\n",
    "pickle.dump(portfolio_list_rnn,open(results_directory+\"/LSTM_RNN_results\",\"wb\"))\n",
    "\n",
    "for filename in os.listdir(results_directory):\n",
    "    files.download(results_directory+\"/\"+filename)  # 多个files.download()语句不能放在同一个代码框中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(model_directory_rnn):\n",
    "    files.download(model_directory_rnn+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory_dnn=\"./model_weights/deep_neural_network\"\n",
    "os.makedirs(model_directory_dnn,exist_ok=True)\n",
    "\n",
    "portfolio_list_dnn=[]\n",
    "for i in range(num_study_period):\n",
    "    print(\"Generating the daily 2k portfolio and corresponding returns of deep neural network for {}-th study period...\".format(i))\n",
    "    trade_start=i*trade_period+train_period\n",
    "    trade_end=(i+1)*trade_period+train_period\n",
    "    portfolio_dict=generate_portfolio_and_return(daily_return.iloc[trade_start:trade_end],\n",
    "                                                 adj_close_price[trade_start:trade_end],i,sequence_length,\n",
    "                                                 \"DNN\",data_directory_dnn,model_directory_dnn)\n",
    "    portfolio_list_dnn.append(portfolio_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory=\"./analysis_results\"\n",
    "os.makedirs(results_directory,exist_ok=True)\n",
    "\n",
    "pickle.dump(portfolio_list_dnn,open(results_directory+\"/DNN_results\",\"wb\"))\n",
    "\n",
    "for filename in os.listdir(results_directory):\n",
    "    files.download(results_directory+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(model_directory_dnn):\n",
    "    files.download(model_directory_dnn+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_raf=\"./train_trade_set/benchmark\"\n",
    "model_directory_raf=\"./model_weights/random_forest\"\n",
    "os.makedirs(model_directory_raf,exist_ok=True)\n",
    "\n",
    "portfolio_list_raf=[]\n",
    "for i in range(num_study_period):\n",
    "    print(\"Generating the daily 2k portfolio and corresponding returns of random forest for {}-th study period...\".format(i))\n",
    "    trade_start=i*trade_period+train_period\n",
    "    trade_end=(i+1)*trade_period+train_period\n",
    "    portfolio_dict=generate_portfolio_and_return(daily_return.iloc[trade_start:trade_end],\n",
    "                                                 adj_close_price[trade_start:trade_end],i,sequence_length,\n",
    "                                                 \"RAF\",data_directory_raf,model_directory_raf)\n",
    "    portfolio_list_raf.append(portfolio_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory=\"./analysis_results\"\n",
    "os.makedirs(results_directory,exist_ok=True)\n",
    "\n",
    "pickle.dump(portfolio_list_rf,open(results_directory+\"/RAF_results\",\"wb\"))\n",
    "\n",
    "for filename in os.listdir(results_directory):\n",
    "    files.download(results_directory+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(model_directory_rf):\n",
    "    files.download(model_directory_rf+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_log=\"./train_trade_set/benchmark\"\n",
    "model_directory_log=\"./model_weights/logistic_regression\"\n",
    "os.makedirs(model_directory_log,exist_ok=True)\n",
    "\n",
    "portfolio_list_log=[]\n",
    "for i in range(num_study_period):\n",
    "    print(\"Generating the daily 2k portfolio and corresponding returns of logistic regression for {}-th study period...\".format(i))\n",
    "    trade_start=i*trade_period+train_period\n",
    "    trade_end=(i+1)*trade_period+train_period\n",
    "    portfolio_dict=generate_portfolio_and_return(daily_return.iloc[trade_start:trade_end],\n",
    "                                                 adj_close_price[trade_start:trade_end],i,sequence_length,\n",
    "                                                 \"LOG\",data_directory_log,model_directory_log)\n",
    "    portfolio_list_log.append(portfolio_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory=\"./analysis_results\"\n",
    "os.makedirs(results_directory,exist_ok=True)\n",
    "\n",
    "pickle.dump(portfolio_list_log,open(results_directory+\"/LOG_results\",\"wb\"))\n",
    "\n",
    "for filename in os.listdir(results_directory):\n",
    "    files.download(results_directory+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(model_directory_log):\n",
    "    files.download(model_directory_log+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_analysis_tables(portfolio_list,daily_return,train_period,trade_period):\n",
    "    \"\"\"\n",
    "    Need debug!!!!!!!\n",
    "    \"\"\"\n",
    "    analysis_tables={}\n",
    "    for k in portfolio_list[0].keys():\n",
    "        analysis_tables[k]=[]\n",
    "    for idx,portfolio_i_period in enumerate(portfolio_list):\n",
    "        for k,portfolio_list in portfolio_i_period.items():\n",
    "            start=idx*trade_period+train_period\n",
    "            end=(idx+1)*trade_period+train_period\n",
    "            portfolio_frame=pd.DataFrame(portfolio_list,index=daily_return.index[start:end])\n",
    "            portfolio_column=pd.DataFrame([idx]*(end-start),index=daily_return.index[start:end],\n",
    "                                          columns=[\"Study period\"])\n",
    "            analysis_tables[k].append(pd.concat([portfolio_frame,portfolio_column],axis=1))\n",
    "    for k,portfolio_list in analysis_tables.items():\n",
    "        analysis_tables[k]=pd.concat(portfolio_list,axis=0)\n",
    "    return analysis_tables\n",
    "\n",
    "analysis_table_dict={}\n",
    "analysis_table_rnn=construct_analysis_tables(portfolio_list_rnn,daily_return,train_period,trade_period)\n",
    "analysis_table_dict[\"RNN\"]=analysis_table_rnn\n",
    "analysis_table_dnn=construct_analysis_tables(portfolio_list_dnn,daily_return,train_period,trade_period)\n",
    "analysis_table_dict[\"DNN\"]=analysis_table_dnn\n",
    "analysis_table_raf=construct_analysis_tables(portfolio_list_raf,daily_return,train_period,trade_period)\n",
    "analysis_table_dict[\"RAF\"]=analysis_table_raf\n",
    "analysis_table_log=construct_analysis_tables(portfolio_list_log,daily_return,train_period,trade_period)\n",
    "analysis_table_dict[\"LOG\"]=analysis_table_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_model_performance(analysis_table_dict,daily_return,return_series=False):\n",
    "    \"\"\"\n",
    "    Need debug!!!!!!\n",
    "    \"\"\"\n",
    "    # create empty DataFrame to record analysis results\n",
    "    index=list(analysis_table_dict.keys())\n",
    "    columns=list(analysis_table_dict[index[0]].keys())\n",
    "    mean_rtn_frame=pd.DataFrame(np.zeros((len(index),len(columns))),index=index,columns=columns)\n",
    "    stdv_rtn_frame=pd.DataFrame(np.zeros((len(index),len(columns))),index=index,columns=columns)\n",
    "    sharpe_r_frame=pd.DataFrame(np.zeros((len(index),len(columns))),index=index,columns=columns)\n",
    "    accuracy_frame=pd.DataFrame(np.zeros((len(index),len(columns))),index=index,columns=columns)\n",
    "    \n",
    "    # conduct analysis\n",
    "    time_series={}\n",
    "    for model_n,analysis_t in analysis_table_dict.items():\n",
    "        time_series[model_n]=[[],[]]\n",
    "        for k,analysis in analysis_t.items():\n",
    "            mean_rtn_frame.loc[model_n,k]=np.mean(analysis[\"return\"])\n",
    "            stdv_rtn_frame.loc[model_n,k]=np.std(analysis[\"return\"])\n",
    "            target=pd.DataFrame((daily_return.values-np.nanmedian(daily_return,axis=1,keepdims=True))>0,\n",
    "                                index=daily_return.index,columns=daily_return.columns)\n",
    "            correct_num=0\n",
    "            for i in len(analysis[\"top_k\"]):\n",
    "                if (k==10) & return_series:\n",
    "                    time_series[model_n][0].extend([1]*k)\n",
    "                    time_series[model_n][1].extend(target.iloc(i)[analysis[\"top_k\"].iloc[i]])\n",
    "                correct_num+=np.sum(target.iloc[i][analysis[\"top_k\"].iloc[i]])\n",
    "                # up_num+=np.sum(target.iloc[i][analysis[\"top_k\"].iloc[i]])\n",
    "            for i in len(analysis[\"flop_k\"]):\n",
    "                if (k==10) & return_series:\n",
    "                    time_series[model_n][0].extend([0]*k)\n",
    "                    time_series[model_n][1].extend(target.iloc[i][analysis[\"flop_k\"].iloc[i]])\n",
    "                correct_num+=np.sum(1-target.iloc[i][analysis[\"flop_k\"].iloc[i]])\n",
    "                # up_num+=np.sum(target.iloc[i][analysis[\"flop_k\"].iloc[i]])\n",
    "            accuracy_frame.loc[model_n,k]=correct_num/(2*k*len(daily_return.index))\n",
    "    \n",
    "    # plot figures\n",
    "    plt.figure(figsize=(20,3*4))\n",
    "    hor_num=len(mean_rtn_frame.columns)\n",
    "    for i in range(hor_num):\n",
    "        plt.subplot(3,hor_num,0*hor_num+i+1):\n",
    "            plt.bar(mean_rtn_frame.index,mean_rtn_frame.iloc[:,i].values)\n",
    "    for i in range(hor_hum):\n",
    "        plt.subplot(3,hor_num,1*hor_num+i+1):\n",
    "            plt.bar(stdv_rtn_frame.index,stdv_rtn_frame.iloc[:,i].values)\n",
    "    for i in range(hor_hum):\n",
    "        plt.subplot(3,hor_num,2*hor_num+i+1):\n",
    "            plt.bar(accuracy_frame.index,accuracy_frame.iloc[:,i].values)   \n",
    "            \n",
    "    return time_series\n",
    " \n",
    "\n",
    "time_series=analysis_model_performance(analysis_table_dict,daily_return,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pesaran_Timmermann_test(target_series,forecaster_series):\n",
    "    \"\"\"\n",
    "    Perform Pesaran Timmermann test given the target series (binary) and forecaster series (binary)\n",
    "    following the guide from http://www.real-statistics.com/time-series-analysis/forecasting-accuracy/pesaran-timmermann-test/\n",
    "    Already passed the unit test!\n",
    "    \"\"\"\n",
    "    p_y=np.mean(target_series)\n",
    "    p_z=np.mean(forecaster_series)\n",
    "    p_yz=np.mean(np.array(traget_series)*np.array(forecaster_series))\n",
    "    n=len(target_series)\n",
    "    \n",
    "    q_y=p_y*(1-p_y)/n\n",
    "    q_z=p_z*(1-p_z)/n\n",
    "    p=p_y*p_z+(1-p_y)*(1-p_z)\n",
    "    v=p*(1-p)/n\n",
    "    w=(2*p_y-1)**2*q_z+(2*p_z-1)**2*q_y+4*q_y*q_z\n",
    "    PT=(p_yz-p)/(v-w)**0.5\n",
    "    p_value=1-scipy.stats.norm(0,1).cdf(PT)\n",
    "    print(\"The p-value of PT statistics is {}\".format(p_value))\n",
    "   \n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diebold_Mariano_test(target_series,forecaster_1,forecaster_2):\n",
    "    \"\"\"\n",
    "    Perform Diebold Mariano test given the target series (binary) and forecasted series (binary) of two models\n",
    "    following the guide from http://www.real-statistics.com/time-series-analysis/forecasting-accuracy/diebold-mariano-test/\n",
    "    Already passed the unit test!\n",
    "    \"\"\"\n",
    "    # transform list to 1-D array\n",
    "    target_array=np.array(target_series)\n",
    "    forecaster_1_array=np.array(forecaster_1)\n",
    "    forecaster_2_array=np.array(forecaster_2)\n",
    "    n=len(target_array)\n",
    "    \n",
    "    # calculate loss-differential series and its auto-covariance\n",
    "    loss_1=target_array-forecaster_1_array\n",
    "    loss_2=target_array-forecaster_2_array\n",
    "    loss_diff=loss_1**2-loss_2**2\n",
    "    loss_acov=stattools.acovf(loss_diff)\n",
    "    \n",
    "    # calculate DM statistics\n",
    "    if n**(1/3)+1-int(n**(1/3)+1)>=0.5:\n",
    "        h=int(n**(1/3)+1)+1\n",
    "    else:\n",
    "        h=int(n**(1/3)+1)\n",
    "    numerator=np.mean(loss_diff)\n",
    "    denominator=((loss_acov[0]+2*np.sum(loss_acov[1:h]))/n)**0.5\n",
    "    DM=numerator/denominator\n",
    "    \n",
    "    # calculate the p-value of DM statistics\n",
    "    if numerator>0:\n",
    "        p_value=(1-scipy.stats.norm(0,1).cdf(DM))*2\n",
    "    else:\n",
    "        p_value=scipy.stats.norm(0,1).cdf(DM)*2\n",
    "    print(\"The p-value of DM statistics is {}\".format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_random_guess(num_pred,accu_pred):\n",
    "    \"\"\"\n",
    "    Calculate the probability of random guess achieving higher accuracy than a given accuracy\n",
    "    It's ok\n",
    "    \"\"\"\n",
    "    prob_success=0.5\n",
    "    miu=num_pred*prob_success\n",
    "    sigma=(num_pred*prob_success*(1-prob_success))**0.5\n",
    "    better_prob=1-scipy.stats.norm(miu,sigma).cdf(int(accu_pred*num_pred))\n",
    "    print(\"The probability of random guess achieving higher accuracy than {} is {}\".format(accu_pred,better_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monkey_throwing_darts(adj_close_price,daily_return,stock_consti,start_idx,end_idx,k,num_iter=100000):\n",
    "    \"\"\"\n",
    "    It's OK but too slow\n",
    "    \"\"\"\n",
    "    merged=pd.merge(daily_return,stock_consti,how=\"left\",left_index=True,right_index=True)\n",
    "    merged.columns=daily_return.columns.append(daily_return.columns)\n",
    "    stock_consti_fillna=merged.iloc[:,len(merged.columns)//2:].fillna(method=\"ffill\")\n",
    "    \n",
    "    daily_return_trunc=daily_return.iloc[start_idx:end_idx]\n",
    "    close_prices_trunc=adj_close_price.iloc[start_idx:end_idx]\n",
    "    stock_consti_trunc=stock_consti_fillna.iloc[start_idx:end_idx]\n",
    "    \n",
    "    return_list=[]\n",
    "    for i in range(len(daily_return_trunc.index)):\n",
    "        close_prices_i=close_prices_trunc.iloc[i]\n",
    "        stock_consti_i=stock_consti_trunc.iloc[i]\n",
    "        selector_1=stock_consti_i[stock_consti_i==1]\n",
    "        daily_return_i=daily_return_trunc.iloc[i]\n",
    "        selector_2=daily_return_i[(1-pd.isna(daily_return_i)).astype(np.bool)]\n",
    "        stock_avail=list(set(selector_1.index) & set(selector_2.index))\n",
    "        daily_return_array=pd.concat([daily_return_i[stock_avail]]*num_iter,axis=1).values.T\n",
    "        close_prices_array=pd.concat([close_prices_i[stock_avail]]*num_iter,axis=1).values.T\n",
    "        select_list_2=[]\n",
    "        select_list_1=[]\n",
    "        for j in range(num_iter):\n",
    "            select_list_2.extend(np.random.permutation(len(stock_avail)))\n",
    "            select_list_1.extend([j]*len(stock_avail))\n",
    "        stock_2k_return=daily_return_array[select_list_1,select_list_2].reshape(num_iter,-1)[:,:2*k]\n",
    "        stock_2k_prices=close_prices_array[select_list_1,select_list_2].reshape(num_iter,-1)[:,:2*k]\n",
    "        stock_2k_prices[:,:k]=-stock_2k_prices[:,:k]\n",
    "        stock_2k_weight=stock_2k_prices/np.sum(stock_2k_prices,axis=1,keepdims=True)\n",
    "        return_i=np.sum(stock_2k_return*stock_2k_weight,axis=1,keepdims=True)  # shape=(num_iter,1)\n",
    "        return_list.append(return_i)\n",
    "    mean_return=np.mean(np.concatenate(return_list,axis=1),axis=1)\n",
    "    \n",
    "    plt.hist(mean_return,bins=100)\n",
    "    \n",
    "start_idx=train_period\n",
    "end_idx=trade_period*num_study_period+train_period\n",
    "%time monkey_throwing_darts(adj_close_price,daily_return,stock_consti,start_idx,end_idx,10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_return_characteristics(analysis_table_dict,k=10):\n",
    "    for model_n,analysis_table in analysis_table_dict.items():\n",
    "        return_total=analysis_table[k][\"return_long\"]+analysis_table[k][\"return_short\"]\n",
    "        return_total.name=\"return_total\"\n",
    "        analysis_table[k]=pd.concat([analysis_table[k],return_total],axis=1)\n",
    "        print(\"Return characteristics of {} before transaction cost\".format(model_n))\n",
    "        print(analysis_table[k][[\"return_total\",\"return_long\",\"return_short\"]].describe())\n",
    "        print(\"Risk characterietics of {} before transaction cost\".format(model_n))\n",
    "        var_1=np.percentile(analysis_table[k][\"return_total\"],0.01)\n",
    "        var_5=np.percentile(analysis_table[k][\"return_total\"],0.05)\n",
    "        cvar_1=np.mean(analysis_table[k][\"return_total\"]*(analysis_table[k][\"return_total\"]<=var_1).astype(np.int16))\n",
    "        cvar_5=np.mean(analysis_table[k][\"return_total\"]*(analysis_table[k][\"return_total\"]<=var_5).astype(np.int16))\n",
    "        print(\"VaR 1% = {}, CVaR 1% = {}, VaR 5% = {}, CVaR 5% = {}\".format(var_1,var_5,cvar_1,cvar_5))\n",
    "        print(\"Return characteristics of {} after transaction cost\".format(model_n))\n",
    "        transaction_cost=0.0005\n",
    "        cost=np.array([[transaction_cost*2,transaction_cost,transaction_cost]]*len(analysis_table[k].index))\n",
    "        print((analysis_table[k][[\"return_total\",\"return_long\",\"return_short\"]]-cost).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
